{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VNEXpress_News_scraper_Colab_Selenium_BeautifulSoup.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOk/uisEfF1vdrq7mQ/y2g5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanctran/web_scrapper_selenium_beautifulsoup/blob/master/VNEXpress_News_scraper_Colab_Selenium_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWfRkVXnxW09",
        "outputId": "076bbb34-dd0b-4d23-99a4-77ef48298294"
      },
      "source": [
        "!pip install selenium\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install webdriver-manager"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 83.2 MB of archives.\n",
            "After this operation, 282 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 89.0.4389.90-0ubuntu0.18.04.2 [1,127 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 89.0.4389.90-0ubuntu0.18.04.2 [73.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 89.0.4389.90-0ubuntu0.18.04.2 [3,809 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 89.0.4389.90-0ubuntu0.18.04.2 [4,697 kB]\n",
            "Fetched 83.2 MB in 3s (24.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-browser (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_89.0.4389.90-0ubuntu0.18.04.2_all.deb ...\n",
            "Unpacking chromium-browser-l10n (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-browser (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-browser-l10n (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting webdriver-manager\n",
            "  Downloading https://files.pythonhosted.org/packages/32/28/a4e7638fc497ff8f86c6670a5f9f42dc018c37a0b254caa5e51799959da5/webdriver_manager-3.3.0-py2.py3-none-any.whl\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting crayons\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/0d/e3fad4ca1de8e70e06444e7d777a5984261e1db98758b5be3e8296c03fe9/crayons-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver-manager) (2.23.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (3.0.4)\n",
            "Installing collected packages: configparser, colorama, crayons, webdriver-manager\n",
            "Successfully installed colorama-0.4.4 configparser-5.0.2 crayons-0.4.0 webdriver-manager-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xcsPijR9xlQI",
        "outputId": "5b477c71-7cb4-489e-fa12-dd4653216854"
      },
      "source": [
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "from signal import signal, SIGINT\n",
        "from sys import exit\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from time import time, sleep\n",
        "import sys\n",
        "import random\n",
        "\n",
        "def handler(signal_received, frame):\n",
        "    # Handle any cleanup here\n",
        "    print('SIGINT or CTRL-C detected.Saving data to file')\n",
        "    df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "    # Export and save the DataFrame df to result.csv file\n",
        "    df.to_csv('result_exit.csv', index=False, encoding='utf_8')\n",
        "    exit(0)\n",
        "\n",
        "signal(SIGINT, handler)\n",
        "\n",
        "print(\"Running. Press Ctrl+C to exit\")\n",
        "\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.headless = True\n",
        "options.add_argument(\"--window-size=1920,1200\")\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')\n",
        "options.add_argument(\"--incognito\")\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver', options=options)\n",
        "\n",
        "data = []\n",
        "de_muc_list = ['thoi-su', 'the-gioi' , 'phap-luat', 'kinh-doanh', 'so-hoa', 'oto-xe-may', 'doi-song', 'van-hoa','giai-tri', 'giao-duc', 'khoa-hoc', 'suc-khoe', 'the-thao', 'du-lich']\n",
        "\n",
        "for i in range(0, len(de_muc_list) - 1 ):\n",
        "    de_muc = de_muc_list[i]\n",
        "    waiting_page = 0\n",
        "    n = 1\n",
        "    found = True\n",
        "    article_dic = {'category':'', 'article_title' : '', 'article_url':'', 'article':'', 'summary':''}\n",
        "    \n",
        "    print(f\"Starting scraping the artice in {de_muc}\")\n",
        "    while found:\n",
        "        article_temp = []\n",
        "        print(f\"Getting page {n} of {de_muc} ...\")\n",
        "        if n == 1:\n",
        "            url = \"https://vnexpress.net/\" + de_muc\n",
        "        else:\n",
        "            url = \"https://vnexpress.net/\"+ de_muc + \"-p\" + str(n)\n",
        "        driver.get(url)\n",
        "        try:\n",
        "            # # Wait until the element with CLASS_NAME = product-item present\n",
        "            # myElem = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'item-news')))\n",
        "\n",
        "            # Wait 30s to poll the DOM element when trying to find any element (or elements) not immediately available\n",
        "            driver.implicitly_wait(30)\n",
        "\n",
        "            print(f\"Page {n} in {de_muc} is ready!\")\n",
        "        except TimeoutException:\n",
        "            print(\"Loading took too much time!\")\n",
        "            print(\"Quitting driver\")\n",
        "            driver.quit()\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        news_list = soup.find_all('article', {'class':'item-news'})\n",
        "        print(f\"Found {len(news_list)} articles in page {n} of {de_muc}\")\n",
        "\n",
        "        if news_list == [] and i != len(de_muc_list) - 1:\n",
        "            found = False\n",
        "            print(\"End of {de_muc}. Move to {de_muc_list[i + 1]}\")\n",
        "        elif news_list == [] and i == len(de_muc_list) - 1:\n",
        "            print(\"Successfully scraping all the category on TuoiTre\")\n",
        "            print(\"Quitting driver\")\n",
        "            found = False\n",
        "            driver.quit()\n",
        "        \n",
        "        for news in news_list:\n",
        "            if news.find('p', {'class':'info-ads'}):\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                article_dic['category'] = de_muc\n",
        "                \n",
        "                article_dic['article_title'] = news.h3.a['title']\n",
        "                print(f\"Scraping: {news.h3.a['title']}\")\n",
        "                \n",
        "                article_href = news.h3.a['href']\n",
        "                article_dic['article_url'] = article_href\n",
        "\n",
        "                driver.get(article_href)\n",
        "                driver.implicitly_wait(30)\n",
        "\n",
        "                article_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                article_dic['summary'] = article_page.find('p', {'class':'description'}).text\n",
        "                \n",
        "                article_ = article_page.find('article', {'class':'fck_detail'}).find_all('p', {\"class\": \"Normal\"})\n",
        "                \n",
        "\n",
        "                article = ''\n",
        "                if article_ != []:\n",
        "                    for item in article_:\n",
        "                        article = article + item.text + \" \"\n",
        "                article_dic['article'] = article\n",
        "            # Handle the error\n",
        "            except Exception as e:\n",
        "                print('We got an error when try to process an article')\n",
        "                tb = sys.exc_info()[2]\n",
        "                print(e.with_traceback(tb))\n",
        "                continue\n",
        "\n",
        "            if article_dic not in data:\n",
        "                article_dic_copy = article_dic.copy()\n",
        "                article_temp.append(article_dic_copy)\n",
        "                print(f\"Scraped {len(article_temp)} articles\")\n",
        "\n",
        "        if article_temp == []:\n",
        "            print(f'Cannot find new article in page {n} of {de_muc}. Waiting page: {waiting_page} pages')\n",
        "            waiting_page += 1\n",
        "\n",
        "        if article_temp == [] and i != len(de_muc_list) - 1 and waiting_page == 5:\n",
        "            print(\"End of {de_muc}. Move to {de_muc_list[i + 1]}\")\n",
        "            found = False\n",
        "            \n",
        "        elif article_temp == [] and i == len(de_muc_list) - 1  and waiting_page == 5:\n",
        "            print(\"Successfully scraping all the category on TuoiTre\")\n",
        "            found = False\n",
        "            \n",
        "\n",
        "        if data != [] and all(elem in article_temp for elem in data) and article_temp != [] :\n",
        "            print(\"Len of data\", len(data))\n",
        "            print(\"Len of article_temp\", len(article_temp))\n",
        "            print('Finish scrapping.')\n",
        "            print('Saving data to result.csv')\n",
        "            # Create the Pandas DataFrame with the collected data       \n",
        "            df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "            # Export and save the DataFrame df to result.csv file\n",
        "            df.to_csv('result.csv', index=False, encoding='utf_8')\n",
        "            print('Successfully saved data to result.csv')\n",
        "            found = False\n",
        "            \n",
        "        print(f\"Starting save {len(article_temp)} articles from article_temp to data\")\n",
        "        print(f\"Data have {len(data)} articles before saving\")\n",
        "        data = data + article_temp\n",
        "        print(f\"Successfully saving {len(article_temp)} articles from article_temp to data\")\n",
        "        print(f\"***********************************************\")\n",
        "        print(f\"* Data have {len(data)} articles after saving *\")\n",
        "        print(f\"***********************************************\")\n",
        "        sleep_time = random.randint(5, 10)\n",
        "        print(f'Scrapper sleep in {sleep_time}')\n",
        "        sleep(sleep_time)\n",
        "        n += 1\n",
        "\n",
        "        print('Saving data to result_temp.csv')\n",
        "        # Create the Pandas DataFrame with the collected data       \n",
        "        df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "        # Export and save the DataFrame df to result.csv file\n",
        "        df.to_csv('result_temp.csv', index=False, encoding='utf_8')\n",
        "        print('Successfully saved data to result_temp.csv')\n",
        "        print(f\"**********************************************************\")\n",
        "        print(f\"* Result_temp.csv have {len(data)} articles after saving *\")\n",
        "        print(f\"**********************************************************\")\n",
        "        sleep(sleep_time)\n",
        "        print(f'Scrapped page {n} of {de_muc}. Continue to page {n+1} of {de_muc} .')\n",
        "        print('')\n",
        "        print(\"##################################################################\")\n",
        "        print('')\n",
        "        \n",
        "\n",
        "\n",
        "# Create the Pandas DataFrame with the collected data       \n",
        "df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "# Export and save the DataFrame df to result.csv file\n",
        "df.to_csv('result.csv', index=False, encoding='utf_8')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running. Press Ctrl+C to exit\n",
            "Starting scraping the artice in thoi-su\n",
            "Getting page 1 of thoi-su ...\n",
            "Page 1 in thoi-su is ready!\n",
            "Found 51 articles in page 1 of thoi-su\n",
            "Scraping: Cháy tàu cá trong đêm\n",
            "Scraped 1 articles\n",
            "Scraping: Quy hoạch hai bên bờ sông Hồng\n",
            "We got an error when try to process an article\n",
            "Message: invalid argument\n",
            "  (Session info: headless chrome=89.0.4389.90)\n",
            "\n",
            "Scraping: Dự thảo quy hoạch sân bay\n",
            "We got an error when try to process an article\n",
            "Message: invalid argument\n",
            "  (Session info: headless chrome=89.0.4389.90)\n",
            "\n",
            "Scraping: Bầu cử đại biểu Quốc hội khóa XV\n",
            "We got an error when try to process an article\n",
            "Message: invalid argument\n",
            "  (Session info: headless chrome=89.0.4389.90)\n",
            "\n",
            "Scraping: Hoạt động dập dịch Covid-19 quý I năm 2021\n",
            "We got an error when try to process an article\n",
            "Message: invalid argument\n",
            "  (Session info: headless chrome=89.0.4389.90)\n",
            "\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "Scraping: Đề án trồng một tỷ cây xanh được phê duyệt\n",
            "Scraped 2 articles\n",
            "Scraping: Thi công sân bay Phan Thiết\n",
            "Scraped 3 articles\n",
            "Scraping: Trình miễn nhiệm 5 Ủy viên thường vụ Quốc hội\n",
            "Scraped 4 articles\n",
            "Scraping: Trình miễn nhiệm Phó chủ tịch nước Đặng Thị Ngọc Thịnh\n",
            "Scraped 5 articles\n",
            "Scraping: 7 công trình sai phép trong một phường ở TP HCM\n",
            "Scraped 6 articles\n",
            "Scraping: Thủ tướng Phạm Minh Chính: 'Phát triển hạ tầng chiến lược có trọng tâm'\n",
            "Scraped 7 articles\n",
            "Scraping: Cá chết nổi trắng hồ nước sát chung cư\n",
            "SIGINT or CTRL-C detected.Saving data to file\n",
            "We got an error when try to process an article\n",
            "list index out of range\n",
            "Scraping: Hà Nội thay thế hàng cây phong lá đỏ\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae51a70750>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Khách chưa khai báo y tế sẽ không được lên máy bay\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496fa9d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Giúp tài xế gom 20 tấn dưa hấu đổ dưới ruộng\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49686690>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cao tốc xuyên Vườn quốc gia Bạch Mã trước ngày thông xe\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4b7367d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Đề cử ông Phạm Minh Chính làm Thủ tướng\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49690550>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Ba chức danh lãnh đạo chủ chốt được giới thiệu với số phiếu 'rất tập trung'\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4969a410>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Ông Trần Tuấn Anh làm Trưởng Ban Kinh tế Trung ương\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49690410>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Ông Võ Văn Thưởng giữ chức Thường trực Ban Bí thư\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49690350>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "We got an error when try to process an article\n",
            "'title'\n",
            "Scraping: Đề xuất của đại biểu với tân Chủ tịch nước\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49690090>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Tân Chủ tịch nước: 'Việt Nam sẽ vượt qua mọi sóng to, gió cả'\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49686f10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cuộc sống bên kênh thoát nước sân bay Tân Sơn Nhất\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496fa7d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Hôm nay Quốc hội bầu Chủ tịch nước, Thủ tướng\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496fa610>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Những 'bài toán khó' của tân Bí thư Hà Nội\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4b736b90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Miền núi phía bắc mưa to\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496faf10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Phát hiện bánh xe lửa hơn 100 năm trước\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496906d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Tai nạn do cuộn thép trên xe rơi xuống đường\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496a5f10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cù lao Gò Gia - nơi sống của hơn 100 hộ dân\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49690210>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Người thân bất lực tiếp cận 4 nạn nhân trong đám cháy\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4ab96550>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Phu gánh dưa ở Tây Nguyên\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4969aad0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Gia Lai giải thể bệnh viện dã chiến điều trị Covid -19\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49686090>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cháy cửa hàng đồ sơ sinh, 4 người tử vong\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496b4f10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: 'Bảo tàng sống' sẽ là điểm nhấn đô thị Đà Nẵng\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49643a90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cháy ôtô ở hầm Tràng Tiền Plaza\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4a482190>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cây phượng đè người phụ nữ tử vong\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae4a2c8fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Bảo dưỡng cầu Long Biên\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496a52d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Thứ trưởng Lê Minh Hoan: 'Dân miền Tây dần thích ứng biến đổi khí hậu'\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496501d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cầu Mỹ Thuận 2 vướng giải phóng mặt bằng\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49650d50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Cần Thơ sụt lún nhất miền Tây\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49bbabd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Lô ma túy giấu trong kiện hàng xuất đi Đài Loan\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae496dde50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Quán cà phê bốc cháy khi đang sửa chữa\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49643950>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Scraping: Tân chủ tịch Quốc hội: 'Mục tiêu tối thượng là hạnh phúc của nhân dân'\n",
            "We got an error when try to process an article\n",
            "HTTPConnectionPool(host='127.0.0.1', port=59999): Max retries exceeded with url: /session/bbb883b8ae1a2a0afc069afa6c3ead4a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fae49661710>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Starting save 7 articles from article_temp to data\n",
            "Data have 0 articles before saving\n",
            "Successfully saving 7 articles from article_temp to data\n",
            "***********************************************\n",
            "* Data have 7 articles after saving *\n",
            "***********************************************\n",
            "Scrapper sleep in 5\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "**********************************************************\n",
            "* Result_temp.csv have 7 articles after saving *\n",
            "**********************************************************\n",
            "SIGINT or CTRL-C detected.Saving data to file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}