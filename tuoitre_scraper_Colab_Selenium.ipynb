{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tuoitre_scraper_Colab_Selenium.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN12md06PBDEdoy9Pe7HM42",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toanctran/web_scrapper_selenium_beautifulsoup/blob/master/tuoitre_scraper_Colab_Selenium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyabXZPGFMrZ",
        "outputId": "ae83657c-2ff5-41b8-a7b5-afc8f3ce03d1"
      },
      "source": [
        "!pip install selenium\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install webdriver-manager"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\r\u001b[K     |▍                               | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 18.6MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 911kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 83.2 MB of archives.\n",
            "After this operation, 282 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 89.0.4389.90-0ubuntu0.18.04.2 [1,127 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 89.0.4389.90-0ubuntu0.18.04.2 [73.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 89.0.4389.90-0ubuntu0.18.04.2 [3,809 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 89.0.4389.90-0ubuntu0.18.04.2 [4,697 kB]\n",
            "Fetched 83.2 MB in 9s (9,114 kB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-browser (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_89.0.4389.90-0ubuntu0.18.04.2_all.deb ...\n",
            "Unpacking chromium-browser-l10n (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_89.0.4389.90-0ubuntu0.18.04.2_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-browser (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Setting up chromium-browser-l10n (89.0.4389.90-0ubuntu0.18.04.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting webdriver-manager\n",
            "  Downloading https://files.pythonhosted.org/packages/32/28/a4e7638fc497ff8f86c6670a5f9f42dc018c37a0b254caa5e51799959da5/webdriver_manager-3.3.0-py2.py3-none-any.whl\n",
            "Collecting crayons\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/0d/e3fad4ca1de8e70e06444e7d777a5984261e1db98758b5be3e8296c03fe9/crayons-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver-manager) (2.23.0)\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver-manager) (1.24.3)\n",
            "Installing collected packages: colorama, crayons, configparser, webdriver-manager\n",
            "Successfully installed colorama-0.4.4 configparser-5.0.2 crayons-0.4.0 webdriver-manager-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGcFEzXeFWLW",
        "outputId": "431ebe71-feff-4575-90f9-6d832523aa47"
      },
      "source": [
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "from signal import signal, SIGINT\n",
        "from sys import exit\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from time import time, sleep\n",
        "import sys\n",
        "import random\n",
        "\n",
        "def handler(signal_received, frame):\n",
        "    # Handle any cleanup here\n",
        "    print('SIGINT or CTRL-C detected.Saving data to file')\n",
        "    df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "    # Export and save the DataFrame df to result.csv file\n",
        "    df.to_csv('result_exit.csv', index=False, encoding='utf_8')\n",
        "    exit(0)\n",
        "\n",
        "signal(SIGINT, handler)\n",
        "\n",
        "print(\"Running. Press Ctrl+C to exit\")\n",
        "\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.headless = True\n",
        "options.add_argument(\"--window-size=1920,1200\")\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')\n",
        "options.add_argument(\"--incognito\")\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver', options=options)\n",
        "\n",
        "data = []\n",
        "de_muc_list = ['thoi-su', 'the-gioi' , 'phap-luat', 'kinh-doanh', ] #'cong-nghe', 'xe', 'nhip-song-tre', 'van-hoa','giai-tri', 'giao-duc', 'khoa-hoc', 'suc-khoe' ]\n",
        "for i in range(0, len(de_muc_list) - 1 ):\n",
        "    de_muc = de_muc_list[i]\n",
        "    waiting_page = 0\n",
        "    n = 1\n",
        "    found = True\n",
        "    article_dic = {'category':'', 'article_title' : '', 'article_url':'', 'article':'', 'summary':''}\n",
        "    \n",
        "    print(f\"Starting scraping the artice in {de_muc}\")\n",
        "    while found:\n",
        "        article_temp = []\n",
        "        print(f\"Getting page {n} of {de_muc} ...\")  \n",
        "        url = \"https://tuoitre.vn/\" + de_muc + \"/trang-\" + str(n) + \".htm\"\n",
        "        driver.get(url)\n",
        "        try:\n",
        "            # # Wait until the element with CLASS_NAME = product-item present\n",
        "            # myElem = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'news-item')))\n",
        "\n",
        "            # Wait 30s to poll the DOM element when trying to find any element (or elements) not immediately available\n",
        "            driver.implicitly_wait(30)\n",
        "\n",
        "            print(f\"Page {n} in {de_muc} is ready!\")\n",
        "        except TimeoutException:\n",
        "            print(\"Loading took too much time!\")\n",
        "            print(\"Quitting driver\")\n",
        "            driver.quit()\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        news_list = soup.find_all('div', {'class':'name-news'})\n",
        "\n",
        "        if news_list == [] and i != len(de_muc_list) - 1:\n",
        "            found = False\n",
        "            print(\"End of {de_muc}. Move to {de_muc_list[i + 1]}\")\n",
        "        elif news_list == [] and i == len(de_muc_list) - 1:\n",
        "            print(\"Successfully scraping all the category on TuoiTre\")\n",
        "            print(\"Quitting driver\")\n",
        "            found = False\n",
        "            driver.quit()\n",
        "            break\n",
        "        \n",
        "        for news in news_list:\n",
        "            try:\n",
        "                article_dic['category'] = de_muc\n",
        "                try:\n",
        "                    article_dic['article_title'] = news.a['title']\n",
        "                except:\n",
        "                    continue\n",
        "                article_href = \"https://tuoitre.vn\" + news.a['href']\n",
        "                article_dic['article_url'] = article_href\n",
        "\n",
        "                driver.get(article_href)\n",
        "                driver.implicitly_wait(30)\n",
        "\n",
        "                article_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                article_dic['summary'] = article_page.find('h2', {'class':'sapo'}).text\n",
        "                \n",
        "                article_ = article_page.find('div', {\"id\": \"main-detail-body\"}).find_all('p')\n",
        "                if article_page.find('div', {'type':'RelatedOneNews'}) != None:\n",
        "                    related_ = article_page.find('div', {'type':'RelatedOneNews'}).p.text\n",
        "\n",
        "                article = ''\n",
        "                if article_ != []:\n",
        "                    for item in article_:\n",
        "                        if related_ != None and item.text == related_:\n",
        "                            continue\n",
        "                        else:\n",
        "                            article = article + item.text + \" \"\n",
        "                article_dic['article'] = article\n",
        "            # Handle the error\n",
        "            except Exception as e:\n",
        "                print('We got an error when try to process an article')\n",
        "                tb = sys.exc_info()[2]\n",
        "                print(e.with_traceback(tb))\n",
        "                continue\n",
        "\n",
        "            if article_dic not in data:\n",
        "                article_dic_copy = article_dic.copy()\n",
        "                article_temp.append(article_dic_copy)\n",
        "                print(f\"Scraped {len(article_temp)} articles\")\n",
        "\n",
        "        if article_temp == []:\n",
        "            print(f'Cannot find new article in page {n} of {de_muc}. Waiting page: {waiting_page} pages')\n",
        "            waiting_page += 1\n",
        "\n",
        "        if article_temp == [] and i != len(de_muc_list) - 1 and waiting_page == 5:\n",
        "            print(\"End of {de_muc}. Move to {de_muc_list[i + 1]}\")\n",
        "            found = False\n",
        "            \n",
        "        elif article_temp == [] and i == len(de_muc_list) - 1  and waiting_page == 5:\n",
        "            print(\"Successfully scraping all the category on TuoiTre\")\n",
        "            found = False\n",
        "            \n",
        "\n",
        "        if data != [] and all(elem in article_temp for elem in data) and article_temp != [] :\n",
        "            print(\"Len of data\", len(data))\n",
        "            print(\"Len of article_temp\", len(article_temp))\n",
        "            print('Finish scrapping.')\n",
        "            print('Saving data to result.csv')\n",
        "            # Create the Pandas DataFrame with the collected data       \n",
        "            df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "            # Export and save the DataFrame df to result.csv file\n",
        "            df.to_csv('result.csv', index=False, encoding='utf_8')\n",
        "            print('Successfully saved data to result.csv')\n",
        "            found = False\n",
        "            \n",
        "        print(f\"Starting save {len(article_temp)} articles from article_temp to data\")\n",
        "        print(f\"Data have {len(data)} articles before saving\")\n",
        "        data = data + article_temp\n",
        "        print(f\"Successfully saving {len(article_temp)} articles from article_temp to data\")\n",
        "        print(f\"Data have {len(data)} articles after saving\")\n",
        "        sleep_time = random.randint(5, 10)\n",
        "        print(f'Scrapper sleep in {sleep_time}')\n",
        "        sleep(sleep_time)\n",
        "        print(f'Scrapped page {n} of {de_muc}. Continue to page {n+1} of {de_muc} .')\n",
        "            \n",
        "        n += 1\n",
        "        print('Saving data to result_temp.csv')\n",
        "        # Create the Pandas DataFrame with the collected data       \n",
        "        df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "        # Export and save the DataFrame df to result.csv file\n",
        "        df.to_csv('result_temp.csv', index=False, encoding='utf_8')\n",
        "        print('Successfully saved data to result_temp.csv')\n",
        "        \n",
        "\n",
        "\n",
        "# Create the Pandas DataFrame with the collected data       \n",
        "df = pd.DataFrame(data=data, columns=data[0].keys())\n",
        "# Export and save the DataFrame df to result.csv file\n",
        "df.to_csv('result.csv', index=False, encoding='utf_8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running. Press Ctrl+C to exit\n",
            "Starting scraping the artice in thoi-su\n",
            "Getting page 1 of thoi-su ...\n",
            "Page 1 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Starting save 15 articles from article_temp to data\n",
            "Data have 0 articles before saving\n",
            "Successfully saving 15 articles from article_temp to data\n",
            "Data have 15 articles after saving\n",
            "Scrapper sleep in 9\n",
            "Scrapped page 1 of thoi-su. Continue to page 2 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 2 of thoi-su ...\n",
            "Page 2 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Starting save 16 articles from article_temp to data\n",
            "Data have 15 articles before saving\n",
            "Successfully saving 16 articles from article_temp to data\n",
            "Data have 31 articles after saving\n",
            "Scrapper sleep in 7\n",
            "Scrapped page 2 of thoi-su. Continue to page 3 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 3 of thoi-su ...\n",
            "Page 3 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 31 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 48 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 3 of thoi-su. Continue to page 4 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 4 of thoi-su ...\n",
            "Page 4 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 48 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 65 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 4 of thoi-su. Continue to page 5 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 5 of thoi-su ...\n",
            "Page 5 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 65 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 82 articles after saving\n",
            "Scrapper sleep in 6\n",
            "Scrapped page 5 of thoi-su. Continue to page 6 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 6 of thoi-su ...\n",
            "Page 6 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 82 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 99 articles after saving\n",
            "Scrapper sleep in 10\n",
            "Scrapped page 6 of thoi-su. Continue to page 7 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 7 of thoi-su ...\n",
            "Page 7 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 99 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 116 articles after saving\n",
            "Scrapper sleep in 7\n",
            "Scrapped page 7 of thoi-su. Continue to page 8 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 8 of thoi-su ...\n",
            "Page 8 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 116 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 133 articles after saving\n",
            "Scrapper sleep in 7\n",
            "Scrapped page 8 of thoi-su. Continue to page 9 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 9 of thoi-su ...\n",
            "Page 9 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 10 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Starting save 15 articles from article_temp to data\n",
            "Data have 133 articles before saving\n",
            "Successfully saving 15 articles from article_temp to data\n",
            "Data have 148 articles after saving\n",
            "Scrapper sleep in 9\n",
            "Scrapped page 9 of thoi-su. Continue to page 10 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 10 of thoi-su ...\n",
            "Page 10 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Starting save 14 articles from article_temp to data\n",
            "Data have 148 articles before saving\n",
            "Successfully saving 14 articles from article_temp to data\n",
            "Data have 162 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 10 of thoi-su. Continue to page 11 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 11 of thoi-su ...\n",
            "Page 11 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 162 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 179 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 11 of thoi-su. Continue to page 12 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 12 of thoi-su ...\n",
            "Page 12 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 179 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 196 articles after saving\n",
            "Scrapper sleep in 10\n",
            "Scrapped page 12 of thoi-su. Continue to page 13 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 13 of thoi-su ...\n",
            "Page 13 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 196 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 213 articles after saving\n",
            "Scrapper sleep in 8\n",
            "Scrapped page 13 of thoi-su. Continue to page 14 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 14 of thoi-su ...\n",
            "Page 14 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 213 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 230 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 14 of thoi-su. Continue to page 15 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 15 of thoi-su ...\n",
            "Page 15 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 230 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 247 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 15 of thoi-su. Continue to page 16 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 16 of thoi-su ...\n",
            "Page 16 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 247 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 264 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 16 of thoi-su. Continue to page 17 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 17 of thoi-su ...\n",
            "Page 17 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 264 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 281 articles after saving\n",
            "Scrapper sleep in 10\n",
            "Scrapped page 17 of thoi-su. Continue to page 18 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 18 of thoi-su ...\n",
            "Page 18 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 281 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 298 articles after saving\n",
            "Scrapper sleep in 6\n",
            "Scrapped page 18 of thoi-su. Continue to page 19 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 19 of thoi-su ...\n",
            "Page 19 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Starting save 16 articles from article_temp to data\n",
            "Data have 298 articles before saving\n",
            "Successfully saving 16 articles from article_temp to data\n",
            "Data have 314 articles after saving\n",
            "Scrapper sleep in 6\n",
            "Scrapped page 19 of thoi-su. Continue to page 20 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 20 of thoi-su ...\n",
            "Page 20 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Starting save 16 articles from article_temp to data\n",
            "Data have 314 articles before saving\n",
            "Successfully saving 16 articles from article_temp to data\n",
            "Data have 330 articles after saving\n",
            "Scrapper sleep in 6\n",
            "Scrapped page 20 of thoi-su. Continue to page 21 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 21 of thoi-su ...\n",
            "Page 21 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 330 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 347 articles after saving\n",
            "Scrapper sleep in 10\n",
            "Scrapped page 21 of thoi-su. Continue to page 22 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 22 of thoi-su ...\n",
            "Page 22 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 347 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 364 articles after saving\n",
            "Scrapper sleep in 9\n",
            "Scrapped page 22 of thoi-su. Continue to page 23 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 23 of thoi-su ...\n",
            "Page 23 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Starting save 16 articles from article_temp to data\n",
            "Data have 364 articles before saving\n",
            "Successfully saving 16 articles from article_temp to data\n",
            "Data have 380 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 23 of thoi-su. Continue to page 24 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 24 of thoi-su ...\n",
            "Page 24 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 380 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 397 articles after saving\n",
            "Scrapper sleep in 10\n",
            "Scrapped page 24 of thoi-su. Continue to page 25 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 25 of thoi-su ...\n",
            "Page 25 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 397 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 414 articles after saving\n",
            "Scrapper sleep in 6\n",
            "Scrapped page 25 of thoi-su. Continue to page 26 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 26 of thoi-su ...\n",
            "Page 26 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 414 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 431 articles after saving\n",
            "Scrapper sleep in 7\n",
            "Scrapped page 26 of thoi-su. Continue to page 27 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 27 of thoi-su ...\n",
            "Page 27 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 431 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 448 articles after saving\n",
            "Scrapper sleep in 8\n",
            "Scrapped page 27 of thoi-su. Continue to page 28 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 28 of thoi-su ...\n",
            "Page 28 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 448 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 465 articles after saving\n",
            "Scrapper sleep in 6\n",
            "Scrapped page 28 of thoi-su. Continue to page 29 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 29 of thoi-su ...\n",
            "Page 29 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 465 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 482 articles after saving\n",
            "Scrapper sleep in 8\n",
            "Scrapped page 29 of thoi-su. Continue to page 30 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 30 of thoi-su ...\n",
            "Page 30 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 482 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 499 articles after saving\n",
            "Scrapper sleep in 7\n",
            "Scrapped page 30 of thoi-su. Continue to page 31 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 31 of thoi-su ...\n",
            "Page 31 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 499 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 516 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 31 of thoi-su. Continue to page 32 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 32 of thoi-su ...\n",
            "Page 32 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Scraped 17 articles\n",
            "Starting save 17 articles from article_temp to data\n",
            "Data have 516 articles before saving\n",
            "Successfully saving 17 articles from article_temp to data\n",
            "Data have 533 articles after saving\n",
            "Scrapper sleep in 5\n",
            "Scrapped page 32 of thoi-su. Continue to page 33 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 33 of thoi-su ...\n",
            "Page 33 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n",
            "Scraped 4 articles\n",
            "Scraped 5 articles\n",
            "Scraped 6 articles\n",
            "Scraped 7 articles\n",
            "We got an error when try to process an article\n",
            "'NoneType' object has no attribute 'text'\n",
            "Scraped 8 articles\n",
            "Scraped 9 articles\n",
            "Scraped 10 articles\n",
            "Scraped 11 articles\n",
            "Scraped 12 articles\n",
            "Scraped 13 articles\n",
            "Scraped 14 articles\n",
            "Scraped 15 articles\n",
            "Scraped 16 articles\n",
            "Starting save 16 articles from article_temp to data\n",
            "Data have 533 articles before saving\n",
            "Successfully saving 16 articles from article_temp to data\n",
            "Data have 549 articles after saving\n",
            "Scrapper sleep in 9\n",
            "Scrapped page 33 of thoi-su. Continue to page 34 of thoi-su .\n",
            "Saving data to result_temp.csv\n",
            "Successfully saved data to result_temp.csv\n",
            "Getting page 34 of thoi-su ...\n",
            "Page 34 in thoi-su is ready!\n",
            "Scraped 1 articles\n",
            "Scraped 2 articles\n",
            "Scraped 3 articles\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}